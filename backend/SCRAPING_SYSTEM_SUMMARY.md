# Sistema de Scraping - Resumen Ejecutivo

## ğŸ“‹ Â¿QuÃ© se implementÃ³?

Se diseÃ±Ã³ e implementÃ³ un **sistema completo de staging para scraping** que almacena TODOS los datos scrapeados antes de publicarlos, con trazabilidad total, deduplicaciÃ³n robusta y gestiÃ³n del pipeline de IA.

## ğŸ—‚ï¸ Archivos Creados

### 1. **MigraciÃ³n SQL**
ğŸ“ `backend/migrations/001_create_scraping_items.sql`

- Esquema completo de la tabla `scraping_items`
- ENUMs para `scraping_status` y `source_media`
- Ãndices optimizados para performance
- Constraints para integridad de datos
- Triggers para auto-actualizaciÃ³n de timestamps
- Vistas Ãºtiles (items pendientes, duplicados, stats)

**CaracterÃ­sticas:**
- âœ… 40+ campos organizados por categorÃ­a
- âœ… DeduplicaciÃ³n por URL y contenido
- âœ… Pipeline de estados (scraped â†’ published)
- âœ… Tracking de IA (modelo, tokens, costo)
- âœ… Manejo de errores y reintentos
- âœ… AuditorÃ­a completa

### 2. **Modelo SQLAlchemy**
ğŸ“ `backend/app/db/models.py` (clase `ScrapingItem`)

- Modelo async compatible con FastAPI
- Mapeo 1:1 con el esquema SQL
- Relaciones con tabla `publications`
- Ãndices compuestos para queries comunes

### 3. **Schemas Pydantic**
ğŸ“ `backend/app/api/schemas.py`

Schemas agregados:
- `ScrapingItemCreate` - Crear nuevo item
- `ScrapingItemUpdate` - Actualizar item
- `ScrapingItemOut` - Response bÃ¡sico
- `ScrapingItemOutDetailed` - Response con todos los campos
- `ScrapingItemFilters` - Filtros para queries
- `PaginatedScrapingItems` - Respuesta paginada
- `ScrapingItemPublishRequest` - Publicar item
- `ScrapingItemStats` - EstadÃ­sticas

### 4. **Endpoints FastAPI**
ğŸ“ `backend/app/api/routes/scraping_items.py`

**Endpoints implementados:**

| MÃ©todo | Endpoint | DescripciÃ³n |
|--------|----------|-------------|
| POST | `/scraping-items` | Crear item (con dedup) |
| POST | `/scraping-items/upsert` | **RECOMENDADO** - Upsert deduplicado |
| GET | `/scraping-items` | Listar con filtros y paginaciÃ³n |
| GET | `/scraping-items/{id}` | Obtener item completo |
| PATCH | `/scraping-items/{id}` | Actualizar item |
| POST | `/scraping-items/{id}/publish` | Crear publicaciÃ³n |
| DELETE | `/scraping-items/{id}` | Eliminar item |
| GET | `/scraping-items/stats/summary` | EstadÃ­sticas |
| POST | `/scraping-items/bulk/mark-duplicates` | Marcar duplicados |

### 5. **Utilidades de DeduplicaciÃ³n**
ğŸ“ `backend/app/scrape/deduplication.py`

Funciones implementadas:
- `normalize_url()` - Normaliza URLs (lowercase, sin tracking params)
- `normalize_content()` - Normaliza contenido (whitespace, lowercase)
- `hash_text()` - SHA-256 de texto
- `generate_url_hash()` - Hash de URL normalizada
- `generate_content_hash()` - Hash de contenido normalizado
- `check_similarity()` - Chequea similitud entre textos

### 6. **Ejemplo de Scraper**
ğŸ“ `backend/examples/scraper_example.py`

Scraper de ejemplo que muestra:
- CÃ³mo scrapear contenido
- CÃ³mo normalizar y hashear
- CÃ³mo usar el endpoint `/upsert`
- Manejo de errores
- Scraping en batch
- DeduplicaciÃ³n en acciÃ³n

### 7. **DocumentaciÃ³n**
ğŸ“ `backend/SCRAPING_ITEMS_README.md`

DocumentaciÃ³n completa que incluye:
- Diagrama de arquitectura
- Schema completo de la tabla
- DescripciÃ³n de todos los campos
- DocumentaciÃ³n de endpoints
- Ejemplos de uso
- Estrategia de deduplicaciÃ³n
- Best practices
- Instrucciones de migraciÃ³n

### 8. **IntegraciÃ³n con Router**
ğŸ“ `backend/app/api/router.py`

- Rutas agregadas bajo `/api/scraping-items`
- Tag: `scraping-items` en Swagger

## ğŸ¯ CaracterÃ­sticas Principales

### 1. **Trazabilidad Total**
Cada item registra:
- De dÃ³nde vino (medio, secciÃ³n, URL)
- QuiÃ©n lo scrapeÃ³ (scraper, versiÃ³n, IP, user-agent)
- CuÃ¡ndo se scrapeÃ³ (timestamp, duraciÃ³n)
- CÃ³mo se procesÃ³ (IA modelo, tokens, costo)
- QuÃ© pasÃ³ con Ã©l (estado, errores, publicaciÃ³n)

### 2. **DeduplicaciÃ³n Robusta**
- **Por URL**: `url_hash` (UNIQUE constraint)
  - Normaliza URLs (remove tracking params, lowercase)
  - Si existe, actualiza contenido (endpoint upsert)

- **Por Contenido**: `content_hash`
  - Normaliza contenido (whitespace, lowercase)
  - Detecta artÃ­culos idÃ©nticos de diferentes URLs
  - Endpoint bulk para marcar duplicados

### 3. **Pipeline de Estados**
```
scraped â†’ pending_review â†’ ready_for_ai â†’ processing_ai â†’
ai_completed â†’ ready_to_publish â†’ published
         â†“
    discarded / error / duplicate
```

### 4. **Manejo de Errores**
- Contador de reintentos (`retry_count`)
- MÃ¡ximo de reintentos configurable (`max_retries`)
- Error trace completo para debugging
- Timestamp del Ãºltimo error
- Estado `error` con mensaje descriptivo

### 5. **Tracking de IA**
- TÃ­tulo, resumen, tags generados por IA
- Modelo utilizado (ej: gpt-4o-mini)
- VersiÃ³n de prompt (para A/B testing)
- Tokens consumidos
- Costo estimado en USD
- Metadata flexible (JSONB)

### 6. **Performance**
- Ãndices optimizados para queries comunes:
  - Por estado (WHERE status = 'ready_for_ai')
  - Por medio y fecha (WHERE source_media = 'lagaceta' ORDER BY article_date)
  - Por hash (deduplicaciÃ³n instantÃ¡nea)
  - Full-text search en tÃ­tulo/contenido (trigram index)

### 7. **Flexibilidad**
- Campo `extra_metadata` (JSONB) para datos especÃ­ficos del scraper
- Campo `ai_metadata` (JSONB) para datos de IA
- Arrays para tags, imÃ¡genes, videos

## ğŸš€ CÃ³mo Usar

### Paso 1: Ejecutar la migraciÃ³n
```bash
psql -U ladatajusta -d ladatajusta -f backend/migrations/001_create_scraping_items.sql
```

### Paso 2: Reiniciar el backend
```bash
cd backend
python -m uvicorn app.main:app --reload
```

### Paso 3: Ver en Swagger
Abrir: http://localhost:8000/docs

VerÃ¡s la secciÃ³n "scraping-items" con todos los endpoints.

### Paso 4: Probar con el ejemplo
```bash
cd backend
python examples/scraper_example.py
```

## ğŸ“Š Endpoints Clave

### Para Scrapers
```python
POST /api/scraping-items/upsert
```
**Uso:** Siempre usar este endpoint para evitar duplicados

### Para Pipeline de IA
```python
GET /api/scraping-items?status=ready_for_ai&limit=100
PATCH /api/scraping-items/{id}  # Actualizar con resultados de IA
```

### Para PublicaciÃ³n
```python
POST /api/scraping-items/{id}/publish
```
**Resultado:** Crea `Publication` y vincula con `publication_id`

### Para Monitoreo
```python
GET /api/scraping-items/stats/summary
```
**Uso:** Dashboard de estadÃ­sticas

## ğŸ”‘ Flujo Completo

1. **Scraper** scrapea noticia â†’ POST `/upsert` â†’ crea `ScrapingItem` con status="scraped"
2. **Revisor** (humano/bot) aprueba â†’ PATCH status="ready_for_ai"
3. **Pipeline IA** procesa â†’ PATCH con ai_title, ai_summary, etc â†’ status="ai_completed"
4. **Sistema** valida â†’ PATCH status="ready_to_publish"
5. **Publicador** crea publicaciÃ³n â†’ POST `/{id}/publish` â†’ crea `Publication` + status="published"

## ğŸ“ˆ Ventajas del Sistema

### vs. Scrapear directo a publicaciones:
âœ… **AuditorÃ­a**: Sabes exactamente quÃ© se scrapeÃ³ y cuÃ¡ndo
âœ… **DeduplicaciÃ³n**: Evitas artÃ­culos repetidos
âœ… **Flexibilidad**: Puedes re-procesar items sin re-scrapear
âœ… **Trazabilidad**: Sabes de dÃ³nde vino cada publicaciÃ³n
âœ… **Control de calidad**: RevisiÃ³n antes de publicar
âœ… **Debugging**: Raw HTML guardado para anÃ¡lisis
âœ… **Costos**: Tracking de tokens/costo de IA

### vs. Tabla simple de scraping:
âœ… **Estados**: Pipeline claro de scraping â†’ publicaciÃ³n
âœ… **Errores**: Manejo robusto de fallos con reintentos
âœ… **Metadatos**: Tracking completo de scraper, IA, etc
âœ… **Performance**: Ãndices optimizados para queries reales
âœ… **Extensibilidad**: JSONB para datos custom

## ğŸ“ Conceptos Clave

### DeduplicaciÃ³n por URL
```python
url_hash = SHA256(normalize_url(original_url))
# Si url_hash existe â†’ actualiza contenido
# Si no existe â†’ crea nuevo item
```

### DeduplicaciÃ³n por Contenido
```python
content_hash = SHA256(normalize_content(content))
# Detecta artÃ­culos idÃ©nticos de diferentes URLs
# Ãštil para syndicated content
```

### Upsert Pattern
```sql
INSERT INTO scraping_items (...)
ON CONFLICT (url_hash) DO UPDATE SET
  content = EXCLUDED.content,
  updated_at = NOW()
RETURNING *;
```

## ğŸ› ï¸ PrÃ³ximos Pasos Sugeridos

1. **Integrar con scrapers existentes**: Modificar scrapers actuales para usar `/upsert`
2. **Pipeline de IA**: Crear worker que procese items con status="ready_for_ai"
3. **Dashboard**: Crear frontend para visualizar stats y gestionar items
4. **AutomatizaciÃ³n**: Scheduler para marcar duplicados periÃ³dicamente
5. **Alertas**: Notificaciones cuando hay muchos errores
6. **Archivado**: Script para archivar items publicados antiguos

## ğŸ“š Recursos

- **SQL Migration**: `backend/migrations/001_create_scraping_items.sql`
- **DocumentaciÃ³n**: `backend/SCRAPING_ITEMS_README.md`
- **Ejemplo de uso**: `backend/examples/scraper_example.py`
- **Swagger/OpenAPI**: http://localhost:8000/docs (secciÃ³n scraping-items)

## âœ… Checklist de ImplementaciÃ³n

- [x] Schema SQL con ENUMs, Ã­ndices y constraints
- [x] Modelo SQLAlchemy async
- [x] Schemas Pydantic completos
- [x] 9 endpoints REST (CRUD + stats + bulk)
- [x] LÃ³gica de upsert deduplicado (PostgreSQL ON CONFLICT)
- [x] Utilidades de normalizaciÃ³n y hashing
- [x] Ejemplo funcional de scraper
- [x] DocumentaciÃ³n completa
- [x] IntegraciÃ³n con router FastAPI
- [ ] Ejecutar migraciÃ³n en DB
- [ ] Probar endpoints
- [ ] Integrar con scrapers existentes
- [ ] Crear pipeline de IA

---

**ğŸ‰ Sistema completo y listo para usar!**

Para preguntas o mejoras, consulta `SCRAPING_ITEMS_README.md` o revisa el cÃ³digo con comentarios detallados.
