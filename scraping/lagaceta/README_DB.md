# Scraper de La Gaceta con Base de Datos

VersiÃ³n mejorada del scraper que guarda directamente en la tabla `scraping_items` de PostgreSQL.

## ğŸ†• Mejoras vs. VersiÃ³n CSV

| CaracterÃ­stica | VersiÃ³n CSV | VersiÃ³n DB |
|----------------|-------------|------------|
| Almacenamiento | CSV local | PostgreSQL |
| DeduplicaciÃ³n | Manual | AutomÃ¡tica (SHA-256 hashes) |
| Pipeline | No | SÃ­ (estados: scraped â†’ ready_for_ai â†’ etc) |
| Trazabilidad | Limitada | Completa (run_id, timestamps, duraciÃ³n) |
| Procesamiento AI | Manual | Integrado en pipeline |
| Manejo de errores | BÃ¡sico | Avanzado (retry, error tracking) |
| Escalabilidad | Baja | Alta (pool de conexiones) |

## ğŸ“‹ Requisitos

### 1. Python y Dependencias

```bash
# Instalar dependencias
pip install -r requirements.txt

# Instalar navegador de Playwright
playwright install chromium
```

### 2. Base de Datos

La tabla `scraping_items` debe existir. Si no existe, ejecutar las migraciones:

```bash
cd ../../backend/migrations
psql -U postgres -d ladatajusta -f 001_create_scraping_items.sql
```

### 3. Variables de Entorno (Opcional)

Crear archivo `.env` en el directorio `scraping/lagaceta/`:

```env
DB_HOST=localhost
DB_PORT=5432
DB_NAME=ladatajusta
DB_USER=postgres
DB_PASSWORD=postgres
```

Si no se configuran, usa valores por defecto (localhost).

## ğŸš€ Uso

### Ejecutar el scraper

```bash
python scrape_lagaceta_db.py
```

### Salida esperada

```
ğŸš€ Iniciando scraper de La Gaceta
ğŸ“¦ Run ID: a1b2c3d4e5f6...
âœ… Conectado a PostgreSQL
âœ… Login exitoso
ğŸ”— 45 enlaces Ãºnicos encontrados

ğŸ“Š Procesando lote 1/9
ğŸ” Procesando: https://www.lagaceta.com.ar/nota/...
âœ… Guardado: Nueva ley de protecciÃ³n animal establece penas mÃ¡s sev...
â­ï¸ Duplicado (URL): https://www.lagaceta.com.ar/nota/...

âœ… Proceso terminado
ğŸ“¦ Run ID: a1b2c3d4e5f6...
ğŸ”Œ ConexiÃ³n a PostgreSQL cerrada
```

## ğŸ” Verificar Datos Scrapeados

### Ver Ãºltimos items scrapeados

```sql
SELECT
    id,
    source_media,
    title,
    status,
    scraped_at,
    article_date
FROM scraping_items
WHERE source_media = 'lagaceta'
ORDER BY scraped_at DESC
LIMIT 10;
```

### EstadÃ­sticas de scraping

```sql
SELECT
    status,
    COUNT(*) as count,
    MIN(scraped_at) as first_scrape,
    MAX(scraped_at) as last_scrape
FROM scraping_items
WHERE source_media = 'lagaceta'
GROUP BY status;
```

### Items listos para procesar con AI

```sql
SELECT
    id,
    title,
    summary,
    scraped_at
FROM scraping_items
WHERE source_media = 'lagaceta'
  AND status = 'scraped'
  AND retry_count < max_retries
ORDER BY article_date DESC
LIMIT 20;
```

## ğŸ“Š Estructura de Datos Guardados

Cada artÃ­culo scrapeado incluye:

### Datos del ArtÃ­culo
- `title`: TÃ­tulo del artÃ­culo
- `summary`: Resumen (si existe)
- `content`: Contenido completo
- `article_date`: Fecha de publicaciÃ³n original
- `source_section`: CategorÃ­a (PolÃ­tica, EconomÃ­a, etc.)

### Metadatos del Scraper
- `source_url`: URL original
- `source_url_normalized`: URL normalizada
- `content_hash`: SHA-256 del contenido (deduplicaciÃ³n)
- `url_hash`: SHA-256 de la URL (deduplicaciÃ³n)
- `scraper_name`: "lagaceta_playwright"
- `scraper_version`: "2.0.0"
- `scraping_run_id`: ID Ãºnico de esta ejecuciÃ³n
- `scraping_duration_ms`: Tiempo que tomÃ³ scrapear este artÃ­culo

### Estado del Pipeline
- `status`: 'scraped' (reciÃ©n scrapeado)
- `status_message`: Mensaje de estado
- `retry_count`: NÃºmero de reintentos (0 inicial)

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Cambiar concurrencia

En `scrape_lagaceta_db.py`, modificar:

```python
CONCURRENCY = 5  # NÃºmero de artÃ­culos a procesar en paralelo
```

âš ï¸ **PrecauciÃ³n**: Valores muy altos pueden sobrecargar el sitio web.

### Cambiar fuente de noticias

Modificar la URL base:

```python
URL_ULTIMO_MOMENTO = "https://www.lagaceta.com.ar/ultimo-momento"
# Otras opciones:
# URL_BASE = "https://www.lagaceta.com.ar/politica"
# URL_BASE = "https://www.lagaceta.com.ar/economia"
```

### Agregar extracciÃ³n de imÃ¡genes

En la funciÃ³n `procesar_noticia()`, agregar:

```python
# Extraer imÃ¡genes
image_urls = await page.eval_on_selector_all(
    "#articleContent img",
    "imgs => imgs.map(img => img.src)"
)

# Luego en data:
'image_urls': image_urls,
```

### Agregar extracciÃ³n de autor

```python
# Buscar elemento de autor (ajustar selector segÃºn La Gaceta)
author = None
if await page.locator(".article-author").is_visible():
    author = await page.locator(".article-author").inner_text()

# Luego en data:
'author': author,
```

## ğŸ”„ Flujo del Pipeline

DespuÃ©s del scraping, los items pasan por este pipeline:

```
scraped â†’ ready_for_ai â†’ processing_ai â†’ ai_completed â†’ ready_to_publish â†’ published
```

Para mover items al siguiente estado, usar:

```sql
-- Marcar como listos para AI
UPDATE scraping_items
SET status = 'ready_for_ai'
WHERE source_media = 'lagaceta'
  AND status = 'scraped'
  AND article_date >= NOW() - INTERVAL '24 hours';
```

## âš ï¸ Troubleshooting

### Error: "relation scraping_items does not exist"

Ejecutar la migraciÃ³n:

```bash
psql -U postgres -d ladatajusta -f ../../backend/migrations/001_create_scraping_items.sql
```

### Error: "asyncpg connection failed"

Verificar:
1. PostgreSQL estÃ¡ corriendo: `pg_ctl status`
2. Credenciales correctas en `.env` o cÃ³digo
3. Firewall/permisos de PostgreSQL

### Error: "playwright not installed"

```bash
playwright install chromium
```

### Duplicados no se detectan

Verificar que los Ã­ndices existan:

```sql
SELECT indexname FROM pg_indexes WHERE tablename = 'scraping_items';
```

Debe incluir: `idx_scraping_items_url_hash` y `idx_scraping_items_content_hash`

## ğŸ“š PrÃ³ximos Pasos

1. **Procesar con AI**: Usar el pipeline para generar tÃ­tulos/resÃºmenes con IA
2. **Crear Publicaciones**: Convertir items `ready_to_publish` en publicaciones
3. **Automatizar**: Crear cron job para scraping periÃ³dico
4. **Expandir**: Adaptar para otras fuentes (ClarÃ­n, Infobae, etc.)

## ğŸ“– Referencias

- [DocumentaciÃ³n de asyncpg](https://magicstack.github.io/asyncpg/)
- [Playwright Python](https://playwright.dev/python/)
- [Pipeline de Scraping (ver migrations)](../../backend/migrations/001_create_scraping_items.sql)
